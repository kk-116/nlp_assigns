# -*- coding: utf-8 -*-
"""Word2Vec.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wgwMxBWCyNmROEWlxvY2rBaGstu2sedb
"""

import numpy as np
import pandas as pd
import re
import nltk
from nltk.corpus import brown
from itertools import chain
from sklearn.model_selection import train_test_split
from sklearn.model_selection import KFold
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sn

from keras_preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Embedding
from keras.layers import Dense, Input, Flatten
from keras.models import Model
from keras.utils.np_utils import to_categorical
from keras.preprocessing.text import Tokenizer

nltk.download('wordnet')
nltk.download('brown')
nltk.download('universal_tagset')

sentences = brown.tagged_sents(tagset='universal')

words = []
tags = []

for sentence in sentences:
    new_sentence = []
    new_tag = []
    for word in sentence:
        new_sentence.append(word[0])
        new_tag.append(word[1])
    words.append(new_sentence)
    tags.append(new_tag)

TEST_SIZE = 0.15
X_train_words, X_test_words, Y_train_tags, Y_test_tags = train_test_split(
    words, tags, test_size=TEST_SIZE, random_state=4)

# instantiate tokeniser
word_tokenizer = Tokenizer(oov_token=True)
tag_tokenizer = Tokenizer()

# fit tokenisers on data
word_tokenizer.fit_on_texts(X_train_words)
tag_tokenizer.fit_on_texts(Y_train_tags)

# use the tokeniser to encode input sequtag_tokenizer = Tokenizer()
train_words_encoded = word_tokenizer.texts_to_sequences(X_train_words)
train_tags_encoded = tag_tokenizer.texts_to_sequences(Y_train_tags)

print(word_tokenizer.word_index)
print(tag_tokenizer.word_index)

maxIndex = max([max(x) for x in train_words_encoded])
cap = int(0.7*maxIndex)
for i in range(len(train_words_encoded)):
    for j in range(len(train_words_encoded[i])):
        if (train_words_encoded[i][j] >= cap):
            train_words_encoded[i][j] = 1

MAX_SEQ_LENGTH = 30
EMBEDDING_SIZE = 100
VOCABULARY_SIZE = len(word_tokenizer.word_index) + 1

train_words_encoded_padded = pad_sequences(
    train_words_encoded, maxlen=MAX_SEQ_LENGTH, padding="post", truncating="post")
train_tags_encoded_padded = pad_sequences(
    train_tags_encoded, maxlen=MAX_SEQ_LENGTH, padding="post", truncating="post")

print(train_words_encoded_padded[0])
print(train_tags_encoded_padded[0])

train_tags_encoded_categorical = to_categorical(train_tags_encoded_padded)

train_tags_encoded_categorical = np.array(train_tags_encoded_categorical)
train_words_encoded_padded = np.array(train_words_encoded_padded)
print(train_words_encoded_padded.shape, train_tags_encoded_categorical.shape)

model = Sequential()
model.add(Embedding(VOCABULARY_SIZE, EMBEDDING_SIZE, input_length=MAX_SEQ_LENGTH))

# Rectified Linear Unit Activation Function
model.add(Dense(128, input_dim=EMBEDDING_SIZE, activation='relu'))
#model.add(Dense(64, activation = 'relu'))
model.add(Dense(13, activation='sigmoid'))

model.compile(loss='binary_crossentropy',
              optimizer='adam', metrics=['accuracy'])
model.summary()

mapIndexToTags = {}
tagCount = 0
for i in tag_tokenizer.word_index:
    mapIndexToTags[tag_tokenizer.word_index[i]] = i
    tagCount += 1
uniqueTags = list(tag_tokenizer.word_index.keys())
print(uniqueTags)


def getConfusionMatrix(prediction, trueTags):
    confusionMatrix = np.zeros((tagCount, tagCount))
    numSentences = prediction.shape[0]
    sentenceLen = prediction.shape[1]

    prediction = np.argmax(prediction, axis=-1)
    trueTags = np.argmax(trueTags, axis=-1)
    for i in range(numSentences):
        for j in range(sentenceLen):
            if (prediction[i][j] == 0):
                break
            confusionMatrix[int(prediction[i][j]) -
                            1][int(trueTags[i][j])-1] += 1
    df_cm = pd.DataFrame(confusionMatrix, index=uniqueTags, columns=uniqueTags)
    # display(df_cm)
    plt.figure(figsize=(15, 10))
    sn.heatmap(df_cm, annot=True, annot_kws={"size": 12})
    plt.show()


kf = KFold(n_splits=5)
kf.get_n_splits(train_words_encoded_padded)

for train_index, valid_index in kf.split(train_words_encoded_padded):
    X_train, X_valid = train_words_encoded_padded[train_index], train_words_encoded_padded[valid_index]
    Y_train, Y_valid = train_tags_encoded_categorical[
        train_index], train_tags_encoded_categorical[valid_index]

    model.fit(X_train, Y_train, validation_data=(
        X_valid, Y_valid), epochs=5, batch_size=128)
    prediction = model.predict(X_valid)
    getConfusionMatrix(prediction, Y_valid)


def vector_representation(word_tokenizer, tag_tokenizer, words, tags, MAX_SEQ_LENGTH):
    # use the tokeniser to encode input sequtag_tokenizer = Tokenizer()
    words_encoded = word_tokenizer.texts_to_sequences(words)
    tags_encoded = tag_tokenizer.texts_to_sequences(tags)

    words_encoded_padded = pad_sequences(
        words_encoded, maxlen=MAX_SEQ_LENGTH, padding="post", truncating="post")
    tags_encoded_padded = pad_sequences(
        tags_encoded, maxlen=MAX_SEQ_LENGTH, padding="post", truncating="post")

    tags_encoded_categorical = to_categorical(tags_encoded_padded)

    tags_encoded_categorical = np.array(tags_encoded_categorical)
    words_encoded_flat = np.array(words_encoded_padded)
    return words_encoded_flat, tags_encoded_categorical


X_test, Y_test = vector_representation(
    word_tokenizer, tag_tokenizer, X_test_words, Y_test_tags, MAX_SEQ_LENGTH)
model.evaluate(X_test, Y_test)


sentence = [input().strip().split()]
vecRep = vector_representation(
    word_tokenizer, tag_tokenizer, sentence, Y_test_tags, MAX_SEQ_LENGTH)[0]
prediction = model.predict(vecRep)
prediction = np.argmax(prediction, axis=-1)[0]
predictedTags = [mapIndexToTags[i] for i in prediction[:len(sentence[0])]]
print(predictedTags)
